# Model Zoo

See image output visualization on [Imagen Museum](https://chromaica.github.io/).

We included more than 30 Models in image synthesis. See the most updated full list here:

Supported Models: [https://github.com/TIGER-AI-Lab/ImagenHub/issues/1](https://github.com/TIGER-AI-Lab/ImagenHub/issues/1)
Supported Metrics: [https://github.com/TIGER-AI-Lab/ImagenHub/issues/6](https://github.com/TIGER-AI-Lab/ImagenHub/issues/6)

## Text-guided Image Generation Model

* [DeepFloydIF](https://www.deepfloyd.ai/deepfloyd-if)
* [Stable Diffusion XL](https://stability.ai/stable-diffusion)
* [Dalle-2](https://openai.com/dall-e-2)
* [OpenJourney](https://openjourney.art/)
* [Stable Diffusion 2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1)

## Mask-guided Image Editing Model

* [SDXL-Inpainting](https://huggingface.co/spaces/diffusers/stable-diffusion-xl-inpainting)
* [SD-Inpainting](https://huggingface.co/runwayml/stable-diffusion-inpainting)
* [GLIDE](https://arxiv.org/abs/2112.10741)
* [BlendedDiffusion](https://omriavrahami.com/blended-diffusion-page/)

## Text-guided Image Editing Model

* [MagicBrush](https://osu-nlp-group.github.io/MagicBrush/)
* [InstructPix2Pix](https://www.timothybrooks.com/instruct-pix2pix)
* [Prompt-to-prompt (Null-text Inversion)](https://null-text-inversion.github.io/)
* [CycleDiffusion](https://arxiv.org/abs/2210.05559)
* [SDEdit](https://sde-image-editing.github.io/)
* [Text2Live](https://text2live.github.io/)
* [DiffEdit](https://arxiv.org/abs/2210.11427)
* [Pix2PixZero](https://pix2pixzero.github.io/)

## Subject-driven Image Generation Model

* [SuTI (Only Images, No code available yet)](https://open-vision-language.github.io/suti/)
* [DreamBooth](https://dreambooth.github.io/)
* [DreamBooth-Lora](https://arxiv.org/abs/2106.09685)
* [BLIP-Diffusion](https://dxli94.github.io/BLIP-Diffusion-website/)
* [Textual Inversion](https://textual-inversion.github.io/)


## Subject-driven Image Editing Model

* [PhotoSwap](https://photoswap.github.io/)
* [DreamEdit](https://dreameditbenchteam.github.io/)
* [BLIP-Diffusion](https://dxli94.github.io/BLIP-Diffusion-website/)

## Multi-concept Image Composition Model

* [CustomDiffusion](https://www.cs.cmu.edu/~custom-diffusion/)
* [DreamEdit](https://dreameditbenchteam.github.io/)
* [Textual Inversion](https://textual-inversion.github.io/)

## Control-guided Image Generation Model

* [ControlNet](https://arxiv.org/abs/2302.05543)
* [UniControl](https://canqin001.github.io/UniControl-Page/)
